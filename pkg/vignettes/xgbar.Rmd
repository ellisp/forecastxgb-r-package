---
title: "Extreme gradient boosting time series forecasting"
author: "Peter Ellis"
date: "6 November 2016"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Extreme gradient boosting time series forecasting}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

The `forecastxgb` package provides time series modelling and forecasting functions that combine the machine learning approach of Chen, He and Benesty's [`xgboost`](https://CRAN.R-project.org/package=xgboost) with the convenient handling of time series and familiar API of Rob Hyndman's [`forecast`](http://github.com/robjhyndman/forecast).  It applies to time series the Extreme Gradient Boosting proposed in [*Greedy Function Approximation: A Gradient Boosting Machine*, by Jerome Friedman in 2001](http://www.jstor.org/stable/2699986). xgboost has become an important machine learning algorithm; nicely explained in [this accessible documentation](http://xgboost.readthedocs.io/en/latest/model.html).

## Basic usage

The workhorse function is `xgbar`.  This fits a model to a time series.  Under the hood, it creates a matrix of explanatory variables based on lagged versions of the response time series, dummy variables for seasons, and numeric time.  That matrix is then fed as the feature set for `xgboost` to do its stuff.
```{r echo = FALSE, cache = FALSE}
set.seed(123)
library(knitr)
knit_hooks$set(mypar = function(before, options, envir) {
    if (before) par(bty = "l", family = "serif")
})
opts_chunk$set(comment=NA, fig.width=7, fig.height=5, cache = FALSE, mypar = TRUE)
```

### Univariate

Usage with default values is straightforward.  Here it is fit to Australian monthly gas production 1956-1995, an example dataset provided in `forecast`:
```{r message = FALSE}
library(forecastxgb)
model <- xgbar(gas)
```
(Note: the "Stopping. Best iteration..." to the screen is produced by `xgboost::xgb.cv`, which uses `cat()` rather than `message()` to print information on its processing.)

By default, `xgbar` uses row-wise cross-validation to determine the best number of rounds of iterations for the boosting algorithm without overfitting.  A final model is then fit on the full available dataset.  The relative importance of the various features in the model can be inspected by `importance_xgb()` or, more conveniently, the `summary` method for objects of class `xgbar`.


```{r}
summary(model)
```
We see in the case of the gas data that the most important feature in explaining gas production is the production 12 months previously; and then other features decrease in importance from there but still have an impact.

Forecasting is the main purpose of this package, and a `forecast` method is supplied.  The resulting objects are of class `forecast` and familiar generic functions work with them.

```{r}
fc <- forecast(model, h = 12)
plot(fc)
```
Note that prediction intervals are not currently available.

### With external regressors
External regressors can be added by using the `xreg` argument familiar from other forecast functions like `auto.arima` and `nnetar`.  `xreg` can be a vector or `ts` object but is easiest to integrate into the analysis if it is a matrix (even a matrix with one column) with well-chosen column names; that way feature names persist meaningfully.  

The example below, with data taken from the `fpp` package supporting Athanasopoulos and Hyndman's [Forecasting Principles and Practice](https://www.otexts.org/fpp) book, shows income being used to explain consumption.  In the same way that the response variable `y` is expanded into lagged versions of itself, each column in `xreg` is expanded into lagged versions, which are then treated as individual features for `xgboost`.

```{r message = FALSE}
library(fpp)
consumption <- usconsumption[ ,1]
income <- matrix(usconsumption[ ,2], dimnames = list(NULL, "Income"))
consumption_model <- xgbar(y = consumption, xreg = income)
summary(consumption_model)
```
We see that the two most important features explaining consumption are the two previous quarters' values of consumption; followed by the income in this quarter; and so on.


The challenge of using external regressors in a forecasting environment is that to forecast, you need values of the future external regressors.  One way this is sometimes done is by first forecasting the individual regressors.  In the example below we do this, making sure the data structure is the same as the original `xreg`.  When the new value of `xreg` is given to `forecast`, it forecasts forward the number of rows of the new `xreg`.  
```{r}
income_future <- matrix(forecast(xgbar(usconsumption[,2]), h = 10)$mean, 
                        dimnames = list(NULL, "Income"))
plot(forecast(consumption_model, xreg = income_future))
```


## Advanced usage
The default settings for `xgbar` give reasonable results.  The key things that can be changed by the user include:

- the maximum number of lags to include as explanatory variables.  There is a trade-off here, as each number higher this gets, the less rows of data you have.  Generally at least two full seasonal cycles are desired, and the default is `max(8, 2 * frequency(y))`.  When the data gets very short this value is sometimes forced lower, with a warning.
- the method for choosing the maximum number of boosting iterations.  The default is row-wise cross validation, after the matrix of lagged explanatory variables has been created.  This is not a traditional approach for cross validation of time series, because the resampling does not preserve the original ordering.  However, the presence of the lagged values means this is less of an issue.  The main alternative (`nrounds_method = "v"`) is to set aside the final 20% of data and use that for validation of the various numbers of rounds of iterations of the first 80% of training data.   Experiments so far suggest that both methods give similar results; if anything the cross-validation method generally recommends a slightly lower number of iterations than does the alternative.

## Options

### Seasonality

Currently there are two methods of treating seasonality.  The first method tried (and still the default, despite its poor performance - this may change) is to throw dummy variables for each season into the mix of features for `xgboost` to work with.  The alternative is to perform classic multiplicative seasonal adjustment on the series before feeding it to `xgboost`.   This seems to work better:

```{r echo = FALSE}
model1 <- xgbar(co2, seas_method = "dummies")
model2 <- xgbar(co2, seas_method = "decompose")
plot(forecast(model1), main = "Dummy variables for seasonality")
plot(forecast(model2), main = "Decomposition seasonal adjustment for seasonality")
```

### Transformations

By default, the data is transformed by a modulus power transformation (as per John and Draper, 1980) before feeding to `xgboost`.  This transformation is similar to a Box-Cox transformation, but works with negative data.  The value of lambda is chosen by `forecass:BoxCox.lambda`, using the Guerrero method.  Setting the `lambda` parameter to 1 will effectively switch off this transformation
```{r echo = FALSE}
model1 <- xgbar(co2, seas_method = "decompose", lambda = 1)
model2 <- xgbar(co2, seas_method = "decompose")
plot(forecast(model1), main = "No transformation")
plot(forecast(model2), main = "With default transformation")
```


## Future developments
Future work might include: 

* additional automated time-dependent features (eg dummy variables for trading days, Easter, etc)
* ability to include xreg values that don't get lagged
* some kind of automated multiple variable forecasting, similar to a vector-autoregression.
* improved treatment of non-stationary series.  At the moment forecasts seem to level off, even though there is a visually obvious trend.  Obviously this could be fixed by differencing, but this would seem to be just reinventing auto.arima.
* Different treatments of seasonality.  At the moment there are two options (dummy variables, or de-seasonalising the series before analysis via classical multiplicative decomposition), but there should be at least two more: ignore seasonality altogether, and add some kind of cyclic time variable.

## Tourism forecasting competition
Here is a more substantive example.  I use the 1,311 datasets from the 2010 Tourism Forecasting Competition described in
 in [Athanasopoulos et al (2011)](http://robjhyndman.com/papers/forecompijf.pdf), originally in the International Journal of Forecasting (2011) 27(3), 822-844.  The data are available in the CRAN package [Tcomp](https://cran.r-project.org/package=Tcomp).  Each data object is a list, with elements inlcuding `x` (the original training data), `h` (the forecasting period) and `xx` (the test data of length `h`).  Only univariate time series are included.
 
To give the `xgbar` model a good test, I am going to compare its performance in forecasting the 1,311 `xx` time series from the matching `x` series with three other modelling approaches:

- Auto-regressive integrated moving average (ARIMA)
- Theta
- Neural networks

Those three are all from Rob Hyndman's `forecast` package.  I am also going to look at the performance of ensembles of the four model types.  With all combinations this means 15 models in total.

Because all four models use the `forecast` paradigm it is relatively straightforward to structure the analysis.  The code below is a little repetitive but should be fairly transparent.  Because of the scale and the embarrassingly parallel nature of the work (ie no particular reason to do it in any particular order, so easy to split into tasks for different processes to do in parallel), I use `foreach` and `doParallel` to make the best use of my 8 logical processors.  The code below sets up a cluster for the parallel computing and a function `competition` which will work on any object of class `Mcomp`, which `Tcomp` inherits from the `Mcomp` package providing the first three "M" forecasting competition data collections.

```{r message = FALSE}
#=============prep======================
library(Tcomp)
library(foreach)
library(doParallel)
library(forecastxgb)
library(dplyr)
library(ggplot2)
library(scales)
```
```{r eval = FALSE}
#============set up cluster for parallel computing===========
cluster <- makeCluster(7) # only any good if you have at least 7 processors :)
registerDoParallel(cluster)

clusterEvalQ(cluster, {
  library(Tcomp)
  library(forecastxgb)
})


#===============the actual analytical function==============
competition <- function(collection, maxfors = length(collection)){
  if(class(collection) != "Mcomp"){
    stop("This function only works on objects of class Mcomp, eg from the Mcomp or Tcomp packages.")
  }
  nseries <- length(collection)
  mases <- foreach(i = 1:maxfors, .combine = "rbind") %dopar% {
    thedata <- collection[[i]]  
    mod1 <- xgbar(thedata$x)
    fc1 <- forecast(mod1, h = thedata$h)
    fc2 <- thetaf(thedata$x, h = thedata$h)
    fc3 <- forecast(auto.arima(thedata$x), h = thedata$h)
    fc4 <- forecast(nnetar(thedata$x), h = thedata$h)
    # copy the skeleton of fc1 over for ensembles:
    fc12 <- fc13 <- fc14 <- fc23 <- fc24 <- fc34 <- fc123 <- fc124 <- fc134 <- fc234 <- fc1234 <- fc1
    # replace the point forecasts with averages of member forecasts:
    fc12$mean <- (fc1$mean + fc2$mean) / 2
    fc13$mean <- (fc1$mean + fc3$mean) / 2
    fc14$mean <- (fc1$mean + fc4$mean) / 2
    fc23$mean <- (fc2$mean + fc3$mean) / 2
    fc24$mean <- (fc2$mean + fc4$mean) / 2
    fc34$mean <- (fc3$mean + fc4$mean) / 2
    fc123$mean <- (fc1$mean + fc2$mean + fc3$mean) / 3
    fc124$mean <- (fc1$mean + fc2$mean + fc4$mean) / 3
    fc134$mean <- (fc1$mean + fc3$mean + fc4$mean) / 3
    fc234$mean <- (fc2$mean + fc3$mean + fc4$mean) / 3
    fc1234$mean <- (fc1$mean + fc2$mean + fc3$mean + fc4$mean) / 4
    mase <- c(accuracy(fc1, thedata$xx)[2, 6],
              accuracy(fc2, thedata$xx)[2, 6],
              accuracy(fc3, thedata$xx)[2, 6],
              accuracy(fc4, thedata$xx)[2, 6],
              accuracy(fc12, thedata$xx)[2, 6],
              accuracy(fc13, thedata$xx)[2, 6],
              accuracy(fc14, thedata$xx)[2, 6],
              accuracy(fc23, thedata$xx)[2, 6],
              accuracy(fc24, thedata$xx)[2, 6],
              accuracy(fc34, thedata$xx)[2, 6],
              accuracy(fc123, thedata$xx)[2, 6],
              accuracy(fc124, thedata$xx)[2, 6],
              accuracy(fc134, thedata$xx)[2, 6],
              accuracy(fc234, thedata$xx)[2, 6],
              accuracy(fc1234, thedata$xx)[2, 6])
    mase
  }
  message("Finished fitting models")
  colnames(mases) <- c("x", "f", "a", "n", "xf", "xa", "xn", "fa", "fn", "an",
                        "xfa", "xfn", "xan", "fan", "xfan")
  return(mases)
}
```

Applying this function to the three different subsets of tourism data (by different frequency) is straightforward but takes a few minutes to run:

```{r eval = FALSE}
#========Fit models==============
system.time(t1  <- competition(subset(tourism, "yearly")))
system.time(t4 <- competition(subset(tourism, "quarterly")))
system.time(t12 <- competition(subset(tourism, "monthly")))

# shut down cluster to avoid any mess:
stopCluster(cluster)
```

The `competition` function returns the mean absolute scaled error (MASE) of every model combination for every dataset.  The following code creates a summary object from the objects `t1`, `t4` and `t12` that hold those individual results:

```{r eval = FALSE}
#==============present results================
results <- c(apply(t1, 2, mean),
             apply(t4, 2, mean),
             apply(t12, 2, mean))

results_df <- data.frame(MASE = results)
results_df$model <- as.character(names(results))
periods <- c("Annual", "Quarterly", "Monthly")
results_df$Frequency <- rep.int(periods, times = c(15, 15, 15))

best <- results_df %>%
  group_by(model) %>%
  summarise(MASE = mean(MASE)) %>%
  arrange(MASE) %>%
  mutate(Frequency = "Average")

Tcomp_results <- results_df %>%
  rbind(best) %>%
  mutate(model = factor(model, levels = best$model)) %>%
  mutate(Frequency = factor(Frequency, levels = c("Annual", "Average", "Quarterly", "Monthly")))
```

The resulting object, `Tcomp_results`, is provided with the `forecastxgb` package.  Visual inspection shows that the average values of MASE provided for the Theta and ARIMA models match those in the [`Tcomp` vignette](https://cran.r-project.org/web/packages/Tcomp/vignettes/tourism-comp.html).  The results are easiest to understand graphically.

```{r, fig.width = 8, fig.height = 6}
leg <- "f: Theta; forecast::thetaf\na: ARIMA; forecast::auto.arima
n: Neural network; forecast::nnetar\nx: Extreme gradient boosting; forecastxgb::xgbar"

Tcomp_results %>%
  ggplot(aes(x = model, y =  MASE, colour = Frequency, label = model)) +
  geom_text(size = 4) +
  geom_line(aes(x = as.numeric(model)), alpha = 0.25) +
  scale_y_continuous("Mean scaled absolute error\n(smaller numbers are better)") +
  annotate("text", x = 2, y = 3.5, label = leg, hjust = 0) +
  ggtitle("Average error of four different timeseries forecasting methods\n2010 Tourism Forecasting Competition data") +
  labs(x = "Model, or ensemble of models\n(further to the left means better overall performance)") +
  theme_grey(9)
```


We see the overall best performing ensemble is the average of the Theta and ARIMA models - the two from the more traditional timeseries forecasting approach.  The two machine learning methods (neural network and extreme gradient boosting) are not as effective, at least in these implementations.  As individual methods, they are the two weakest, although the extreme gradient boosting method provided in `forecastxgb` performs noticeably better than `forecast::nnetar`.

Theta by itself is the best performing with the annual data - simple methods work well when the dataset is small and highly aggregate.  The best that can be said of the `xgbar` approach in this context is that it doesn't damage the Theta method much when included in a combination - several of the better performing ensembles have `xgbar` as one of their members.  In contrast, the neural network models do badly with this collection of annual data.

Adding `auto.arima` and `xgbar` to an ensemble of quarterly or monthly data definitely improves on Theta by itself.  The best performing single model for quarterly or monthly data is `auto.arima` followed by `thetaf`.  Again, neural networks are the poorest of the four individual models.

Overall, I conclude that with univariate data, `xgbar` has little to add to an ensemble that already contains `auto.arima` and `thetaf` (or - not shown - the closely related `ets`).  I believe however that inclusion of `xreg` external regressors would shift the balance in favour of `xgbar` and maybe even `nnetar` - the more complex and larger the dataset, the better the chance that these methods will have something to offer.  If and when I find a large collection of timeseries competition data with external regressors I will probably add a second vignette, or at least a blog post at [http://ellisp.github.io](http://ellisp.github.io).